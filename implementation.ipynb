{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust detrending, rereferencing, outlier detection, and inpainting, for-multichannel-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of [1] in Python and experiments on real time series.  \n",
    "Esteban Christiann (ENS Paris-Saclay) and Alexi Canesse (ENS de Lyon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "from meegkit.utils.sig import stmcb # Steiglitz-McBride iteration method for ringing removal\n",
    "from alphacsc import learn_d_z\n",
    "try:\n",
    "    from alphacsc.utils import construct_X\n",
    "except:\n",
    "    from alphacsc.utils.convolution import construct_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Load MEG data extracted from [2] *(See the end of the notebook for the data extraction)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_channels = [3, 12, 15, 19, 22]\n",
    "FS = 2400 # Sampling frequency = 2400Hz\n",
    "\n",
    "meg_data = np.load(\"data/MEG.npy\")\n",
    "\n",
    "print(meg_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three channel synthetic data with linear dependancy (according to model 2 in [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 300\n",
    "t, x1, _ = utils.generate_noisy_polynomial(degree=10, noise_level=0.2, num_points=num_points)\n",
    "_, x2, _ = utils.generate_noisy_polynomial(degree=10, noise_level=0.2, num_points=num_points)\n",
    "x3 = x1 + x2 + np.random.normal(scale=0.2, size=[num_points])\n",
    "x = np.stack([x1, x2, x3], axis=1)\n",
    "x_not_corrupted = x.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some corruption to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.ones([num_points, 3], dtype=bool)\n",
    "w[50:75, 0] = False\n",
    "w[100:150, 1] = False\n",
    "w[200:220, 2] = False\n",
    "x[~w] = 10\n",
    "x1, x2, x3 = x.T\n",
    "\n",
    "utils.plot_signals_side_by_side(t, x.T, x_not_corrupted.T, y_names1=[\"x1\", \"x2\", \"x3\"], y_names2=[\"x1\", \"x2\", \"x3\"], title1=\"Corruped signal\", title2=\"Not corrupted signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic data with steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data with steps \n",
    "sigma = 1e-1\n",
    "x_step = np.random.randn(3, num_points) * sigma\n",
    "# Add steps\n",
    "for signal in x_step:\n",
    "    step_locations = utils.generate_random_numbers(np.random.randint(3, 10), 10, num_points - 9, 15)\n",
    "    for step_location in step_locations:\n",
    "        signal[step_location:] += 1. + np.random.random()\n",
    "utils.plot_signals(t, x_step, title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Robust detrending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_detrend(x, order, w=None, basis='polynomials', thresh=0.5, maxiter=20):\n",
    "     \"\"\"\n",
    "     Robustly removes trend from data.\n",
    "\n",
    "     Parameters:\n",
    "     - x: raw data\n",
    "     - order: order of polynomial or number of sin/cosine pairs\n",
    "     - w: weights\n",
    "     - basis: 'polynomials' [default] or 'sinusoids', or user-provided matrix\n",
    "     - thresh: threshold for outliers [default: .5 sd]\n",
    "     - maxiter: number of iterations [default: 20]\n",
    "\n",
    "     Returns:\n",
    "     - original - trend: detrended data\n",
    "     - trend: trend\n",
    "     - w: updated weights\n",
    "     - r: basis matrix used\n",
    "     \"\"\"\n",
    "     # Generate basis matrix\n",
    "     if isinstance(basis, np.ndarray):\n",
    "          r = basis\n",
    "     else:\n",
    "          lin = np.linspace(-1, 1, x.shape[0])\n",
    "          if basis == 'polynomials':\n",
    "               r = np.column_stack([lin ** k for k in range(0, order + 1)])\n",
    "          elif basis == 'sinusoids':\n",
    "               r = np.column_stack([np.sin(2 * np.pi * k * lin / 2) for k in range(0, order + 1)] +\n",
    "                                   [np.cos(2 * np.pi * k * lin / 2) for k in range(0, order + 1)])\n",
    "          else:\n",
    "               raise ValueError(\"Invalid basis type\")\n",
    "\n",
    "     # Initialize weights if not provided\n",
    "     if w is None:\n",
    "          if len(x.shape) == 1:\n",
    "               w = np.ones_like(x, dtype=bool)[:, np.newaxis]\n",
    "          else:\n",
    "               w = np.ones_like(x, dtype=bool)[:]\n",
    "     \n",
    "     # If the data is multichannel, the algorithm is applied to each channel independently \n",
    "     trend = x.copy()\n",
    "     original = x.copy()\n",
    "     if len(trend.shape) == 1:\n",
    "          trend = trend[:, np.newaxis]\n",
    "          original = original[:, np.newaxis]\n",
    "     for _ in range(maxiter):\n",
    "          # If the data is multichannel, the algorithm is applied to each channel independently \n",
    "          for dim in range(trend.shape[1]):\n",
    "               # Fit to basis\n",
    "               coefficients, _, _, _ = np.linalg.lstsq(r[w[:, dim],:], original[w[:, dim], dim], rcond=None)\n",
    "               trend[:,dim] = r @ coefficients\n",
    "               # Update weights\n",
    "               d = np.abs(trend[:, dim] - original[:, dim])\n",
    "\n",
    "               new_w = d < thresh * np.std(d)\n",
    "               if (new_w == w[:, dim]).all():\n",
    "                    # Algorithm converged, early stopping\n",
    "                    break\n",
    "               w[:, dim] = new_w\n",
    "\n",
    "\n",
    "     return original - trend, trend, w, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the algorithm on our generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrended_x1, trend_x1, weights_x1, _ = robust_detrend(x1, 10, thresh=1)\n",
    "utils.plot_signals(t, [x1, detrended_x1[:, 0], weights_x1, trend_x1], y_names=[\"signal\", \"detrended\", \"weights\", \"fit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the algorithm on real MEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_chan = meg_data[:, 22]\n",
    "detrended_meg, trend_meg, _, _ = robust_detrend(meg_chan, 10, thresh=1)\n",
    "utils.plot_signals_side_by_side(np.arange(meg_chan.shape[0]), [meg_chan, trend_meg], [detrended_meg[:, 0]], y_names1=[\"signal\", \"fit\"], y_names2=[\"detrended\"], alpha1=[.75, .2, 1.], sharey=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_chan = meg_data[:, 3]\n",
    "detrended_meg, trend_meg, _, _ = robust_detrend(meg_chan, 10, thresh=1)\n",
    "utils.plot_signals_side_by_side(np.arange(meg_chan.shape[0]), [meg_chan, trend_meg], [detrended_meg[:, 0]], y_names1=[\"signal\", \"fit\"], y_names2=[\"detrended\"], alpha1=[.75, .2, 1.], sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def inpaint(x, w, keep_valid=True):\n",
    "    \"\"\"\n",
    "    Reconstruct data if corrupted samples are known\n",
    "\n",
    "    Parameters:\n",
    "    - x: Nchannels x Nsamples raw data\n",
    "    - w: Nchannels x Nsamples bool matrix indicating corruptions\n",
    "    - keep_valid: set to true to reconstruct only corrupted samples\n",
    "\n",
    "    Returns:\n",
    "    - new_x: reconstructed data\n",
    "    \"\"\"\n",
    "    if w is None:\n",
    "        w = np.ones_like(x, dtype=bool)\n",
    "\n",
    "    N = x.shape[1] # Number of channels\n",
    "    new_x = x.copy()\n",
    "    \n",
    "    # Optimization trick: represent bool matrix using 1 bit for each coefficient\n",
    "    packed_w = np.packbits(w, axis=1)\n",
    "    full_pattern, full_partition = np.unique(packed_w, axis=0, return_inverse=True)\n",
    "    full_pattern = np.unpackbits(full_pattern, axis=1, count=N).astype(bool)\n",
    "\n",
    "    for n in tqdm(range(N)):\n",
    "\n",
    "        # Partition the time axis using the state of other channels\n",
    "        pattern = full_pattern.copy()\n",
    "        pattern[:, n] = False \n",
    "        pattern = np.packbits(pattern, axis=1) # Optimization again...\n",
    "        pattern, partition = np.unique(pattern, axis=0, return_inverse=True)\n",
    "        pattern = np.unpackbits(pattern, axis=1, count=N).astype(bool)\n",
    "        partition = partition[full_partition]\n",
    "        \n",
    "        K = pattern.shape[0]\n",
    "\n",
    "        for k in range(K):\n",
    "\n",
    "            T_k = partition == k\n",
    "            Tprime_k = np.logical_and(w[:, n], T_k) # Timestamps we use to estimate the projection\n",
    "            \n",
    "            Tinpaint = np.logical_and(~w[:, n], T_k) # Timestamps to reconstruct\n",
    "            if not keep_valid:\n",
    "                Tinpaint = T_k\n",
    "\n",
    "            if Tinpaint.any():\n",
    "                xother_Tp = x[Tprime_k, :][:, pattern[k]]\n",
    "                ones = np.ones([xother_Tp.shape[0], 1], dtype=np.float32)\n",
    "                xother_Tp = np.concatenate([xother_Tp, ones], axis=1)\n",
    "\n",
    "                xother_Ti = x[Tinpaint, :][:, pattern[k]]\n",
    "                ones =  np.ones([xother_Ti.shape[0], 1], dtype=np.float32)\n",
    "                xother_Ti = np.concatenate([xother_Ti, ones], axis=1)\n",
    "                \n",
    "                # Estimate the coefficients and reconstuct the data\n",
    "                coefs = np.linalg.lstsq(xother_Tp, x[Tprime_k, n], rcond=None)[0]\n",
    "                new_x[Tinpaint, n] = xother_Ti @ coefs\n",
    "\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inpaint = inpaint(x, w)\n",
    "\n",
    "utils.plot_signals_side_by_side(t, x_inpaint.T, x.T, title1=\"Inpainted\", title2=\"Original\")\n",
    "\n",
    "to_plot = np.concatenate([x_inpaint, x_inpaint[:, [0]] + x_inpaint[:, [1]]], axis=1)\n",
    "utils.plot_signals_side_by_side(t, to_plot.T, [x_inpaint[:, 0] + x_inpaint[:, 1] - x_not_corrupted[:,2], x[:,2]], y_names1=[\"x1\", \"x2\", \"x3\", \"x1 + x2\"], y_names2=[\"Difference between not corrupted and reconstructed\", \"Corrupted\"], title1=\"Reconstruction of x3 using inpainted signals\", title2=\"Reconstruction of x3 using inpainted signals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detection(x, thres=2., maxiter=20):\n",
    "    \"\"\"\n",
    "    Flag outliers using the inpaint algorithm\n",
    "\n",
    "    Parameters:\n",
    "    - x: Nchannels x Nsamples raw data\n",
    "    - thres: threshold for outlier detection (default: 2 stddev)\n",
    "    - maxiter: maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - w: Nchannels x Nsamples bool matrix where False indicates an outlier\n",
    "    \"\"\"\n",
    "    w = np.ones_like(x, dtype=bool) # Initially assume there are no outliers\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        # Try to reconstruct the data using the inpainting algorithm\n",
    "        xbar = inpaint(x, w, keep_valid=False)\n",
    "\n",
    "        # Flag high reconstruction errors as outliers\n",
    "        d = np.abs(x - xbar)\n",
    "        new_w = ~(d > thres * d.std(axis=0, keepdims=True))\n",
    "\n",
    "        print(f\"[Outlier] Iteration {it+1}/{maxiter}: {100 * (~new_w).sum() / new_w.size:.2f}% flagged as outliers\")\n",
    "        \n",
    "        # Early stopping if the algorithm has converged\n",
    "        if (w == new_w).all():\n",
    "            print(f\"[Outlier] Done.\")\n",
    "            return w\n",
    "        w = new_w\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_w = outlier_detection(x)\n",
    "utils.plot_signals_side_by_side(t, x.T, estimated_w.T, title1=\"Signal\", title2=\"estimated weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Robust Rereferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_rereferencing(x, w=None):\n",
    "    \"\"\"\n",
    "    Perform robust referencing on the input signal.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input signal.\n",
    "    - w (numpy.ndarray): Weights from outlier detection.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Robustly referenced signal.\n",
    "    \"\"\"\n",
    "    if w is None:\n",
    "        w = outlier_detection(x)\n",
    "    robust_mean = np.mean(x, where=w)\n",
    "    return x - robust_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying it on our generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rereferenced_x = robust_rereferencing(x)\n",
    "utils.plot_signals_side_by_side(t, rereferenced_x.T, rereferenced_x.T - (x_not_corrupted.T - np.mean(x_not_corrupted)), title1=\"Rereferenced\", title2=\"Difference with non corrupted rereferenced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_steps(x, thresh, guard, depth):\n",
    "    \"\"\"\n",
    "    Find step glitch\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Data to clean.\n",
    "    - thresh (float): Threshold\n",
    "    - guard (int): Minimum duration of stable interval in samples .\n",
    "    - depth (int): Recursion depth, determines the number of steps.\n",
    "\n",
    "    Returns:\n",
    "    - stepList (list): Indices of steps.\n",
    "    \"\"\"\n",
    "    if depth == 0 or x.shape[0] <= 2 * guard:\n",
    "        return []\n",
    "\n",
    "    T = x.shape[0] # x is a single channel here\n",
    "\n",
    "    M0 = np.cumsum(x) / np.arange(1, T+1)\n",
    "    V0 = np.cumsum(np.square(x)) / np.arange(1, T+1)\n",
    "    V0 -= np.square(M0)\n",
    "    V0 *= np.arange(1, T+1)\n",
    "\n",
    "    MT = np.cumsum(x[::-1]) / np.arange(1, T+1)\n",
    "    VT = np.cumsum(np.square(x[::-1])) / np.arange(1, T+1)\n",
    "    VT -= np.square(MT)\n",
    "    VT *= np.arange(1, T+1)\n",
    "    VT = VT[::-1]\n",
    "    \n",
    "    t0 = np.argmin((V0 + VT)[guard : T-guard])\n",
    "    steps = [guard + t0]\n",
    "    \n",
    "    # Check if the step is relevent \n",
    "    if (V0[t0] + VT[t0]) / V0[-1] > thresh:\n",
    "        return []\n",
    "\n",
    "    if depth and steps:\n",
    "        steps_left = find_steps(x[:steps[0]], thresh=thresh, guard=guard, depth=depth-1)\n",
    "        # Add an offset because x[steps[0] + 1] becomes index 0\n",
    "        steps_right = steps[0] + find_steps(x[steps[0]+1:], thresh=thresh, guard=guard, depth=depth-1)\n",
    "        steps = np.concatenate((steps_left, steps, steps_right))\n",
    "\n",
    "    return steps.astype(int)\n",
    "\n",
    "def step_removal(x, thresh=0.7, guard=5, depth=5):\n",
    "    \"\"\"\n",
    "    Remove step glitch\n",
    "    \n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Data to clean (time * channels).\n",
    "    - thresh (float): Threshold (default: 0.7).\n",
    "    - guard (int): Minimum duration of stable interval in samples (default: 5).\n",
    "    - depth (int): Recursion depth (default: 5), determines the number of steps.\n",
    "\n",
    "    Returns:\n",
    "    - y (numpy.ndarray): Step-removed data.\n",
    "    - stepList (list): Indices of steps.\n",
    "    \"\"\"\n",
    "\n",
    "    y = x.copy()\n",
    "\n",
    "    all_stepList = []\n",
    "    for chan in range(x.shape[1]):\n",
    "        # Find step indices\n",
    "        stepList = find_steps(x[:, chan], thresh=thresh, guard=guard, depth=depth)\n",
    "\n",
    "        all_stepList.append(stepList)\n",
    "\n",
    "        if len(stepList):\n",
    "            stepList = [0] + list(stepList) + [x.shape[0]]\n",
    "            for split in range(1, len(stepList) - 1):\n",
    "                y1 = y[stepList[split - 1] + 1 : stepList[split] - 1, chan]  # plateau before\n",
    "                y2 = y[stepList[split] + 1 : stepList[split + 1] - 1, chan]  # plateau after\n",
    "                step = np.mean(y2) - np.mean(y1)\n",
    "                y[stepList[split] + 1:, chan] -= step\n",
    "\n",
    "    return y, all_stepList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the algorithm on our generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_step_removed, stepList = step_removal(x_step[[1]].T, thresh=0.7)\n",
    "utils.plot_signals(t, [x_step[[1]].T, x_step_removed], y_names=[\"Step signal\", \"de-stepped signal\"], y_lines=[t[step] for steps in stepList for step in steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the algorithm on real MEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that steps are removed but ringing still present..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_chan = meg_data[:, [19]]\n",
    "meg_nostep, step_list = step_removal(meg_chan)\n",
    "utils.plot_signals_side_by_side(range(meg_data.shape[0]), y1=[meg_chan, meg_nostep], y2=[meg_nostep], y_names1=[\"Step signal\", \"de-stepped signal\"], y_names2=\"de-stepped signal\", title1=\"Signal de de-step\", title2=\"de-stepped signal\", sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ringing removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ringing_removal(x, step_list):\n",
    "    \"\"\"\n",
    "    Reduce ringing effect caused by the antialiasing filter response to steps\n",
    "\n",
    "    Paramters:\n",
    "    - x: (Nsamples, Nchannels) data matrix\n",
    "    - step_list: list of step indexes (can be found with step removal algorithm)\n",
    "\n",
    "    Return:\n",
    "    - new_x: cleaned data\n",
    "    \"\"\"\n",
    "    N = x.shape[1]\n",
    "    n_num, n_den = 8, 8\n",
    "    n_samples = 100\n",
    "\n",
    "    new_x = x.copy()\n",
    "    for n in range(N):\n",
    "        for step in step_list[n]:\n",
    "            ringing = x[step : step+n_samples, n]\n",
    "            ringing = ringing - ringing.mean()\n",
    "            if ringing.shape[0] < n_samples:\n",
    "                break\n",
    "            ringing[n_samples//2 : ] = 0\n",
    "            b, a = stmcb(ringing, q=n_num, p=n_den, niter=10)\n",
    "\n",
    "            impulse = np.arange(n_samples) == 0\n",
    "            model = lfilter(b, a, impulse)\n",
    "            new_x[step : step+n_samples, n] -= model\n",
    "\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the algorithm on our generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some filter\n",
    "[b, a] = butter(6, 0.2)\n",
    "\n",
    "x = np.arange(300) == 0 # An impulse...\n",
    "x = lfilter(b, a, x) * 100 # ... goes through the filter\n",
    "x = np.roll(x, 50) + np.random.normal(size=300)\n",
    "x = x[:, None]\n",
    "t = np.linspace(0, 1, x.shape[0])\n",
    "\n",
    "# We already kown that the problem is at timestep 50, but with real data we would\n",
    "# get the timesteps from the step removal algorithm\n",
    "x_noring = ringing_removal(x, [[50]])\n",
    "utils.plot_signals(t, [x, x_noring], y_names=[\"Original signal with ringing\", \"Cleaned signal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the algorithm on real MEG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the output of the step removal algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_nostep_noring = ringing_removal(meg_nostep, step_list)\n",
    "\n",
    "utils.plot_signals(range(meg_chan.shape[0]), meg_nostep_noring.T, title=\"After ringing removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 343_440, 343_640\n",
    "utils.plot_signals(range(start, end), [meg_nostep[start:end], meg_nostep_noring[start:end]], title=\"Zoom on some ringing artefact\", y_names=[\"After step removal and before ringing removal\", \"After ringing removal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the ringing removal algorithm, the signal is cleaner. Data is ready for outlier detection and inpainting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction using dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [270, 102, 19, 125, 25, 35, 24, 159, 13, 262, 28, 80, 204, 192, 211, 11]\n",
    "meg = meg_data[342_000:352_000, channels]\n",
    "meg = (meg - meg.mean(axis=0, keepdims=True)) / meg.std(axis=0, keepdims=True)\n",
    "meg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg_clean, step_list = step_removal(meg)\n",
    "meg_clean = ringing_removal(meg_clean, step_list)\n",
    "\n",
    "meg_clean = robust_detrend(meg_clean, 20)[0]\n",
    "\n",
    "w = outlier_detection(meg_clean, thres=4)\n",
    "meg_clean = inpaint(meg_clean, w)\n",
    "\n",
    "meg_clean = robust_rereferencing(meg_clean, w)\n",
    "meg_clean = meg_clean - meg_clean.mean(axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, start=0, end=None):\n",
    "    T, N = x.shape\n",
    "    if end is None:\n",
    "        end = T\n",
    "\n",
    "    fig, axes = plt.subplots(N, sharex=True, figsize=(16, 16))\n",
    "    for n in range(N):\n",
    "        ax = axes[n]\n",
    "        ax.plot(x[start:end, n])\n",
    "    fig.show()\n",
    "\n",
    "plot(meg[::10])\n",
    "plot(meg_clean[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg = meg[:, [0]]\n",
    "meg_clean = meg_clean[:, [0]]\n",
    "\n",
    "std = meg_clean.std()\n",
    "meg /= std\n",
    "meg_clean /= std\n",
    "\n",
    "utils.plot_signals_side_by_side(range(meg.shape[0]), y1=meg.T, y2=meg_clean.T, y_names1=[\"Original signal\"], y_names2=[\"Clean signal\"], sharey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_atoms = 5  # K\n",
    "atom_length = 20  # L\n",
    "penalty = 2. # lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning a dictionary and codes\n",
    "pobj, _, d_hat, z_hat, _ = learn_d_z(X=meg.T, n_atoms=n_atoms, n_times_atom=atom_length,\n",
    "    reg=penalty, n_iter=20, n_jobs=8, verbose=1)\n",
    "\n",
    "pobj_clean, _, d_hat_clean, z_hat_clean, _ = learn_d_z(X=meg_clean.T, n_atoms=n_atoms, n_times_atom=atom_length,\n",
    "    reg=penalty, n_iter=20, n_jobs=8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruction on the original signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 4500, 5000\n",
    "\n",
    "# Reconstruction with the dictionary and the sparse codes\n",
    "reconstruction = construct_X(z_hat, d_hat).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "tt = np.arange(meg.shape[0])\n",
    "ax.plot(tt[start:end], meg[start:end], label=\"original\", alpha=0.5)\n",
    "ax.plot(tt[start:end], reconstruction[start:end], label=\"reconstructed from original\")\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re construction on the signal with step removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction with the dictionary and the sparse codes\n",
    "reconstruction_clean = construct_X(z_hat_clean, d_hat_clean).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "tt = np.arange(meg_clean.shape[0])\n",
    "ax.plot(tt[start:end], meg_clean[start:end], label=\"original with step removed\", alpha=0.5)\n",
    "ax.plot(tt[start:end], reconstruction_clean[start:end], label=\"Cleaned signal\")\n",
    "\n",
    "_ = plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error on both reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reconstruction MSE (original): {np.mean((meg - reconstruction)**2):.3e}\")\n",
    "print(f\"Reconstruction MSE (step removed): {np.mean((meg_clean - reconstruction_clean)**2):.3e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting real MEG signals (DO NOT RUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download raw data\n",
    "MEG signals from [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution, this is a big file \n",
    "# !wget https://figshare.com/ndownloader/files/6509115\n",
    "# !unzip phantom090715_BrainampDBS_20150709_07.ds.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import MEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "file = \"data/phantom090715_BrainampDBS_20150709_07.ds\"\n",
    "raw = mne.io.read_raw_ctf(file, preload=True)\n",
    "raw = raw.pick_types(meg=True, eeg=False, eog=False)\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw.get_data().T\n",
    "data /= data.std()\n",
    "np.save(\"data/MEG.npy\", np.float32(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Alain de Cheveigné, Dorothée Arzounian,\n",
    "Robust detrending, rereferencing, outlier detection, and inpainting for multichannel data,\n",
    "NeuroImage,\n",
    "Volume 172,\n",
    "2018,\n",
    "Pages 903-912,\n",
    "ISSN 1053-8119,\n",
    "https://doi.org/10.1016/j.neuroimage.2018.01.035.\n",
    "(https://www.sciencedirect.com/science/article/pii/S1053811918300351)\n",
    "\n",
    "[2] Vladimir Litvak. \n",
    "2016. \n",
    "Magnetoencephalography (MEG) recordings from a phantom with Deep Brain Stimulation (DBS) artefacts. \n",
    "DOI:https://doi.org/10.6084/m9.figshare.4042911.v3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
